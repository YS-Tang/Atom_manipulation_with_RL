{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single atom training\n",
    "This notebook goes through the workflow of setting the hyperparameters, collecting atom manipulation data, and training the deep reinforcement learning agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r'D:\\tys\\Atom_manipulation_with_RL')\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque, namedtuple\n",
    "import torch\n",
    "from AMRL import RealExpEnv, Episode_Memory, Createc_Controller, sac_agent, ReplayMemory, HerReplayMemory\n",
    "from AMRL.Environment.Env_new_tys import RealExpEnv\n",
    "from AMRL import plot_graph, show_reset, show_done, show_step\n",
    "matplotlib.rcParams['image.cmap'] = 'gray'\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the parameters and create a RealExpEnv object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "succeed to connect\n",
      "succeed to connect\n",
      "Load cnn weight\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "step_nm = 0.4 #Set the radius of the manipulation start position relative the the atom start position\n",
    "goal_nm  = 1 #Set the radius of the manipulation end position relative the the atom start position\n",
    "max_mvolt = 20 #Set the maximum bias voltage in mV \n",
    "max_pcurrent_to_mvolt_ratio = 4500 #Set the maximum conductance in pA/mV\n",
    "max_step = 10 #Set maximum episode length\n",
    "#Set the path to load CNN weight for the atom movement classifier\n",
    "CNN_weight_path = 'atom_move_detector.pth'\n",
    "current_jump  = 4 #Set the current jump gradient/ std(current) threshold required to take STM scan\n",
    "\n",
    "#Set STM scan parameters\n",
    "pixel = 128\n",
    "im_size_nm = 15.2 #Image size in nm \n",
    "scan_mV = 2000 #bias voltage\n",
    "\n",
    "createc_controller = Createc_Controller(None, None, None, None)\n",
    "x_nm, y_nm = createc_controller.get_offset_nm()\n",
    "offset_nm = np.array([x_nm, y_nm]) #Set offset to current offset value\n",
    "\n",
    "#Set manipulation parameters to pull atoms from image edge to center\n",
    "# pull_back_mV = 5 #bias in mV\n",
    "# pull_back_pA = 60000 #current in pA\n",
    "\n",
    "#Set manipulation limit [left, right, up, down] in nm\n",
    "manip_limit_nm = np.array([x_nm - 0.5*im_size_nm+0.25, x_nm + 0.5*im_size_nm-0.25, y_nm+0.25, y_nm+im_size_nm-0.25])\n",
    "\n",
    "env = RealExpEnv(step_nm=step_nm, \n",
    "                 max_mvolt=max_mvolt, \n",
    "                 max_pcurrent_to_mvolt_ratio=max_pcurrent_to_mvolt_ratio, \n",
    "                 goal_nm=goal_nm, \n",
    "                 current_jump=current_jump, \n",
    "                 im_size_nm=im_size_nm, \n",
    "                 offset_nm=offset_nm, \n",
    "                 manip_limit_nm=manip_limit_nm, \n",
    "                 pixel=pixel, \n",
    "                 scan_mV=scan_mV, \n",
    "                 max_step=max_step, \n",
    "                 load_weight='atom_move_detector.pth', \n",
    "                 random_scan_rate = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a sac_agent object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "batch_size= 8 #Set minibatch size\n",
    "LEARNING_RATE = 0.0003 #Set learning rate\n",
    "\n",
    "#Set the action space range\n",
    "ACTION_SPACE = namedtuple('ACTION_SPACE', ['high', 'low'])\n",
    "action_space = ACTION_SPACE(high = torch.tensor([1,1,1,1,1,1]), low = torch.tensor([-1,-1,-1,-1,1/3,1/2]))\n",
    "\n",
    "#Initialize the soft actor-critic agent\n",
    "agent = sac_agent(num_inputs = 4, num_actions = 6, action_space = action_space, device=device, hidden_size=256, lr=LEARNING_RATE,\n",
    "                 gamma=0.9, tau=0.005, alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a HerReplayMemory object\n",
    "Here we use the hindsight experience replay with the 'future' strategy to sample goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "replay_size=1000000 #Set memory size\n",
    "\n",
    "memory = HerReplayMemory(replay_size, env, strategy = 'future')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Episode_Memory object\n",
    "The episode memory class is used to store all the relavant information in each training episode, including the STM images, state, action, reward, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_memory = Episode_Memory()\n",
    "#TODO\n",
    "#Set the folder name to store training data and neural network weight\n",
    "folder_name = r'D:\\tys\\scripts\\AMRL' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the hyperparameters for Emphasize Recent Experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_k_min = 500\n",
    "eta = 0.994\n",
    "max_ep_len = max_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create empty lists for logging performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards, alphas, precisions, episode_lengths = [], [], [], []\n",
    "avg_episode_rewards, avg_alphas, avg_precisions, avg_episode_lengths = [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sac_train(max_steps = max_step, num_episodes = 50, episode_start = 0):\n",
    "    \"\"\"\n",
    "    Collect training data and train the RL agent\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_steps: int\n",
    "            maximum steps in an episode\n",
    "            \n",
    "    num_episodes: int\n",
    "            Train for this many episodes\n",
    "    \n",
    "    episode_start: int\n",
    "            Index to use for the starting episode\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None : None\n",
    "    \"\"\"\n",
    "    for i_episode in range(episode_start,episode_start+num_episodes):\n",
    "        print('Episode:', i_episode)\n",
    "        episode_reward, episode_steps = 0, 0\n",
    "        done = False\n",
    "        state, info = env.reset(update_conv_net=False)\n",
    "        show_reset(env.img_info, env.atom_start_absolute_nm, env.destination_absolute_nm)\n",
    "        episode_memory.update_memory_reset(env.img_info, i_episode, info)\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state)\n",
    "            old_atom_nm = env.atom_absolute_nm\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            episode_steps+=1\n",
    "            episode_reward+=reward\n",
    "            mask = float(not done)\n",
    "            memory.push(state,action,reward,next_state,mask)\n",
    "            episode_memory.update_memory_step(state, action, next_state, reward, done, info)\n",
    "            show_step(env.img_info, info['start_nm']+old_atom_nm, info['end_nm']+old_atom_nm,\n",
    "                        env.atom_absolute_nm, env.atom_start_absolute_nm, \n",
    "                        env.destination_absolute_nm, action[4]*env.max_mvolt, \n",
    "                        action[5]*env.max_pcurrent_to_mvolt_ratio*action[4]*env.max_mvolt)\n",
    "            print('step:', step,'reward', reward, 'precision:', env.dist_destination)\n",
    "            if done:\n",
    "                episode_memory.update_memory_done(env.img_info, env.atom_absolute_nm, env.atom_relative_nm)\n",
    "                episode_memory.save_memory(folder_name)\n",
    "                print('Episode reward:', episode_reward)\n",
    "                break\n",
    "            else:                \n",
    "                state=next_state\n",
    "             \n",
    "        if (len(memory)>batch_size):\n",
    "            episode_K = int(episode_steps)\n",
    "            for k in range(episode_K):\n",
    "                c_k = max(int(memory.__len__()*eta**((k)*(1000/episode_K))), 500)\n",
    "                agent.update_parameters(memory, batch_size, c_k)\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        alphas.append(agent.alpha.item())\n",
    "        precisions.append(env.dist_destination)\n",
    "        episode_lengths.append(episode_steps)\n",
    "        avg_episode_rewards.append(np.mean(episode_rewards[-min(100,len(episode_rewards)):]))\n",
    "        avg_alphas.append(np.mean(alphas[-min(100, len(alphas)):]))\n",
    "        avg_precisions.append(np.mean(precisions[-min(100, len(precisions)):]))\n",
    "        avg_episode_lengths.append(np.mean(episode_lengths[-min(100, len(episode_lengths)):]))\n",
    "        \n",
    "        if (i_episode+1)%2==0:\n",
    "            plot_graph(episode_rewards, precisions, alphas, episode_lengths,\n",
    "                      avg_episode_rewards, avg_alphas, avg_precisions, avg_episode_lengths)\n",
    "            \n",
    "        if (i_episode)%20 == 0:\n",
    "            torch.save(agent.critic.state_dict(), '{}/_critic_{}.pth'.format(folder_name,i_episode))\n",
    "            torch.save(agent.policy.state_dict(), '{}/_policy_{}.pth'.format(folder_name,i_episode))\n",
    "            torch.save(agent.alpha, '{}/_alpha_{}.pth'.format(folder_name,i_episode))\n",
    "            torch.save(env.atom_move_detector.conv.state_dict(), '{}/_atom_move_detector_conv_{}.pth'.format(folder_name,i_episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "The scan will take 83.9 seconds\n",
      "图像漂移: [0. 0.]纳米\n",
      "atom_absolute_nm_f: [[ 21.22808059 359.24450266]\n",
      " [ 21.68886016 354.30650084]]\n",
      "atom_absolute_nm_b: [[ 20.95247734 359.23405614]\n",
      " [ 21.43719228 354.27579195]]\n",
      "atom_relative_nm_f: [[8.84632778 5.59749758]\n",
      " [9.30710735 0.65949576]]\n",
      "atom_relative_nm_b: [[8.57072453 5.58705106]\n",
      " [9.05543947 0.62878686]]\n",
      "out of range: False\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\tys\\Atom_manipulation_with_RL\\AMRL\\Environment\\get_atom_coordinate_tys.py:275: FutureWarning: `binary_dilation` is deprecated since version 0.26 and will be removed in version 0.28. Use `skimage.morphology.dilation` instead. Note the lack of mirroring for non-symmetric footprints (see docstring notes).\n",
      "  r = morphology.binary_dilation(maxima, footprint=diamond)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "待改为切换其他原子继续",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msac_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36msac_train\u001b[39m\u001b[34m(max_steps, num_episodes, episode_start)\u001b[39m\n\u001b[32m     22\u001b[39m episode_reward, episode_steps = \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m     23\u001b[39m done = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m state, info = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdate_conv_net\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m show_reset(env.img_info, env.atom_start_absolute_nm, env.destination_absolute_nm)\n\u001b[32m     26\u001b[39m episode_memory.update_memory_reset(env.img_info, i_episode, info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\tys\\Atom_manipulation_with_RL\\AMRL\\Environment\\Env_new_tys.py:91\u001b[39m, in \u001b[36mRealExpEnv.reset\u001b[39m\u001b[34m(self, update_conv_net)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m.out_of_range(\u001b[38;5;28mself\u001b[39m.atom_absolute_nm, \u001b[38;5;28mself\u001b[39m.inner_limit_nm))\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.out_of_range(\u001b[38;5;28mself\u001b[39m.atom_absolute_nm, \u001b[38;5;28mself\u001b[39m.inner_limit_nm):\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33m待改为切换其他原子继续\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m#goal_nm is set between 0.28 - 2 nm\u001b[39;00m\n\u001b[32m     94\u001b[39m goal_nm = \u001b[38;5;28mself\u001b[39m.lattice_constant + np.random.random()*(\u001b[38;5;28mself\u001b[39m.goal_nm - \u001b[38;5;28mself\u001b[39m.lattice_constant)\n",
      "\u001b[31mNotImplementedError\u001b[39m: 待改为切换其他原子继续"
     ]
    }
   ],
   "source": [
    "sac_train(episode_start = 0,num_episodes = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any([False, False])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
